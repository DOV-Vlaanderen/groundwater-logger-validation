---
title: "Hydrostatic pressure"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    fig_width: 7
    fig_height: 4
    toc: true
    toc_depth: 4
    number_sections: false
    df_print: paged # default, kable, tibble
    fig_caption: true
  pdf_document:
    toc: true
bibliography: bibliography.bib
vignette: >
  %\VignetteIndexEntry{User documentation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
loadNamespace("data.table")
```

This is an advanced guide. It explains how the `detect_` functions work for hydrostatic pressure. Make sure you have first read the getting started material.

## Hydrostatic pressure model

We use the following model for hydrostatic pressure:

$$
z_t = z_{t-1} + \epsilon(\Delta t)
$$

In literature this also goes by the name of a pure random walk: the next point starts from the previous with a random deviation $\epsilon(\Delta t)$.

### Error function

The error $\epsilon$ is a function of time difference between $t$ and $t-1$. The underlying idea is that the larger this timeinterval $\Delta t$, the larger the variance of $\epsilon$. Based on $\epsilon$ analysis of "correct" hysdrostatic pressure timeseries, the following density gradients are extracted in function of $\Delta t$.

```{r eval=FALSE, echo=FALSE}
samples <- sapply(as.character(seq(5, 1*60*24*4, by = 5)*60), function(interval.sec) {
  gwloggeR:::apriori.hydropressure.difference.samples(as.numeric(interval.sec))
}, simplify = FALSE, USE.NAMES = TRUE)

df <- data.table::rbindlist(lapply(names(samples), function(interval.sec) {
  data.table::data.table('TIMEDIFF' = as.numeric(interval.sec),
                         'VALUE' = unlist(unname(samples[[interval.sec]])))
}), use.names = TRUE, idcol = FALSE)

data.table::setkey(df, TIMEDIFF)

saveRDS(df, file = './hydropressure/df.rds')

df.ecdf <- df[, .(ECDF = list(ecdf(VALUE))), by = TIMEDIFF]
data.table::setkey(df.ecdf, TIMEDIFF)

# x = TIMEDIFF, y = VALUE
cumulative.density <- function(x, y) {
  data.table::rbindlist(Vectorize(function(x, y) {
    data.frame('CDF' = df.ecdf[TIMEDIFF == x, ECDF][[1]](y), x, y)
  }, vectorize.args = 'x', SIMPLIFY = FALSE)(x, y))
}

df.grad <- cumulative.density(x = df.ecdf[, TIMEDIFF], y = seq(-150, 150, length.out = 1000))
saveRDS(df.grad, file = './hydropressure/df_grad.rds')
```

```{r, echo=FALSE}
df.grad <- readRDS('./hydropressure/df_grad.rds')
df.quant <- readRDS('./hydropressure/df.rds')[
    , .(Q.025 = quantile(VALUE, 0.025),
        Q.975 = quantile(VALUE, 0.975)),
    by = .(x = TIMEDIFF)]

ggplot2::ggplot(data = df.grad, mapping = ggplot2::aes(x = x/60/60, y = y)) +
  ggplot2::geom_raster(mapping = ggplot2::aes(fill = CDF), interpolate = TRUE, alpha = 0.8) +
  ggplot2::scale_fill_gradient2(low = 'white', mid = 'red', high = 'white', midpoint = 0.5) +
  ggplot2::xlab('TIMEDIFF (hour)') + ggplot2::ylab('VALUEDIFF (cmH2O)') +
  ggplot2::ggtitle('Cumulative density gradient of aprior hydrostatic pressure data\nin function of time with 95 % confidence interval.') +
  ggplot2::geom_line(data = df.quant, mapping = ggplot2::aes(y = Q.975), col = 'red') +
  ggplot2::geom_line(data = df.quant, mapping = ggplot2::aes(y = Q.025), col = 'red') +
  ggplot2::scale_x_continuous(breaks = seq(0, 200, by = 24.833/2)) + # lunar day: 24h50
  ggplot2::theme_minimal()
```

### Distribution of errors

Based on the "correct" _a-priori_ timeseries, the densities do not seem to be normal. For example, for $\Delta t = 5 \text{min}$ we have:

```{r hydropressure-5min-hist, echo=FALSE}
x <- gwloggeR:::apriori.hydropressure.difference.samples(5*60)
ggplot2::ggplot(data = data.frame('x' = x), 
                mapping = ggplot2::aes_string(x = 'x')) +
  ggplot2::geom_histogram(mapping = ggplot2::aes(y = ..density..), fill = 'black', bins = 100) +
  ggplot2::ylab('Density') +
  ggplot2::theme_light()
```

On the other hand, without tidal effect, the normality seems much more pronounced. For example, $\Delta t = 12 \text{h} 25 \text{min}$:

```{r hydropressure-12h-hist, echo=FALSE}
x <- gwloggeR:::apriori.hydropressure.difference.samples(60*60*12 + 60*25)
ggplot2::ggplot(data = data.frame('x' = x), 
                mapping = ggplot2::aes_string(x = 'x')) +
  ggplot2::geom_histogram(mapping = ggplot2::aes(y = ..density..), fill = 'black', bins = 100) +
  ggplot2::ylab('Density') +
  ggplot2::theme_light()
```

This suggest that the model should be adjusted for tidal effects.

## Extensions

The main idea is that $z_t$ is polluted by different kinds of effects. We define each of these effects seperatey. Inspiration of these definitions is mainly taken from the works of Fox (1972) and more recent papers by Chen and Pedro.

### Outliers

An additive outlier (AO) at time $t_{AO}$ is defined as an exogenous change of a single observation. 

$$
x_t = z_t + \omega I(t = t_{AO})
$$

Note that $x_t$ is the observed value, and $z_t$ is the underlying process as defined previously. Rewriting this in function of $x$ we get:

$$
x_t = x_{t-1} + \epsilon(\Delta t) + \omega \left[I(t=t_{AO}) - I(t = t_{AO} + 1) \right]
$$

### Levelshifts

A levelshift (LS) is an exogenous change that lasts.

$$
x_t = z_t + \frac{\omega I(t = t_{LS})}{1-B}
$$

Note that $B$ is the back operator. Rewriting this in $x$ gives a more comprehensive form:

$$
x_t = x_{t-1} + \epsilon(\Delta t) + \omega I(t=t_{LS})
$$

### Temporal changes

A temporal change (TC) is an effect that decays exponentially with factor $\delta$.

$$
x_t = z_t + \frac{\omega I(t = t_{LS})}{1-\delta B}
$$

Rewritten in $x$ we have:

$$
x_t = x_{t-1} + \epsilon(\Delta t) + \omega \left[ I(t=t_{TC}) - \frac{(1-\delta)I(t = t_{TC} + 1)}{1 - \delta B} \right]
$$

## Likelihood optimization

Based on AO, LS and TC written in function of $x$ only, one sees that the difference $x_t - x_{t-1} = \epsilon (\Delta t)$ unless AO, LS or TC occured. Thus our first step in optimization is to take the differences $x_t - x_{t-1}$ and test how likely they are under the distribution of $\epsilon (\Delta t)$. If very unlikely (cf. TODO), then consider the observation a candidate for AO, LS or TC.

These three effects can come in all kinds of combinations. It seems difficult, if not impossible, to write a decent if-else structure to catch all the possible configurations. That is why we choose a likelihood based approach.

### Likelihood function

Distribution of errors, for likelihood optimization, is assumed normal. This is because normal PDF (mostly) results in a convex likelihood.

### Likelihood ratio testing

At this point, we only know $t$'s which are candidates for AO, LS and TC. But we do not know which is which. That is why we test them all. For testing we use the likelihood ratio (LR) testing. (cf. TODO)

Note that in case of TC we have two parameters: $\omega$ and $\delta$. $\delta$ varies between $0$ and $1$. If $\delta = 0$ we have an outlier (AO), and if $\delta = 1$ we have a levelshift (LS). How do we choose between a TC with $\delta = 0$ and AO? As we will see later, we use likelihood ratio testing: since TC uses two parameters to explain the same effect as AO with one parameter, an AO is more likely. Thus, a TC will only be chosen if $\delta$ is significanly different from $0$ and $1$. 

### Examples

This case shows nicely the "creativity" of the procedure in selecting the effects...

## Future work

* Adjust model for tidal effects.
* Better $\epsilon (\Delta t)$ estimation than current +10 % of min/max value.
* Revise LR test treshold.
* Use _a-priori_ data more as a prior and estimate $\epsilon(\Delta t)$ with data at hand.
* Extract AO, LS and TC if `verbose = TRUE`.

## References


