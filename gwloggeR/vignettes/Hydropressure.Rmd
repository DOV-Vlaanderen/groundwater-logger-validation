---
title: "Hydrostatic pressure"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    fig_width: 7
    fig_height: 4
    toc: true
    toc_depth: 4
    number_sections: false
    df_print: paged # default, kable, tibble
    fig_caption: true
  pdf_document:
    toc: true
bibliography: bibliography.bib
vignette: >
  %\VignetteIndexEntry{Hydrostatic pressure}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
loadNamespace("data.table")
```

This is an advanced guide. It explains how the [`detect_`](../reference/index.html#section-detection-functions) functions work for hydrostatic pressure data. Make sure you have first read the [getting started](gwloggeR.html) material.

## Hydrostatic pressure model

We assume that the hydrostatic pressure behaves according to the following model:

$$
z_t = z_{t-1} + \epsilon(\Delta t)
$$

In literature this also goes by the name of a pure random walk: the next point starts from the previous and jumps with a random deviation $\epsilon(\Delta t)$.

### Error function

The error $\epsilon$ is a function of time difference between $t$ and $t-1$. The underlying idea is that the larger this time-interval $\Delta t$, the larger the variance of $\epsilon$. Based on $\epsilon$ analysis of "correct" _a-priori_ hydrostatic pressure timeseries, the following density gradients are extracted in function of $\Delta t$.

```{r eval=FALSE, echo=FALSE}
samples <- sapply(as.character(seq(5, 1*60*24*4, by = 5)*60), function(interval.sec) {
  gwloggeR:::apriori.hydropressure.difference.samples(as.numeric(interval.sec))
}, simplify = FALSE, USE.NAMES = TRUE)

df <- data.table::rbindlist(lapply(names(samples), function(interval.sec) {
  data.table::data.table('TIMEDIFF' = as.numeric(interval.sec),
                         'VALUE' = unlist(unname(samples[[interval.sec]])))
}), use.names = TRUE, idcol = FALSE)

data.table::setkey(df, TIMEDIFF)

saveRDS(df, file = './hydropressure/df.rds')

df.ecdf <- df[, .(ECDF = list(ecdf(VALUE))), by = TIMEDIFF]
data.table::setkey(df.ecdf, TIMEDIFF)

# x = TIMEDIFF, y = VALUE
cumulative.density <- function(x, y) {
  data.table::rbindlist(Vectorize(function(x, y) {
    data.frame('CDF' = df.ecdf[TIMEDIFF == x, ECDF][[1]](y), x, y)
  }, vectorize.args = 'x', SIMPLIFY = FALSE)(x, y))
}

df.grad <- cumulative.density(x = df.ecdf[, TIMEDIFF], y = seq(-150, 150, length.out = 1000))
saveRDS(df.grad, file = './hydropressure/df_grad.rds')
```

```{r, echo=FALSE}
df.grad <- readRDS('./hydropressure/df_grad.rds')
df.quant <- readRDS('./hydropressure/df.rds')[
    , .(Q.025 = quantile(VALUE, 0.025),
        Q.975 = quantile(VALUE, 0.975)),
    by = .(x = TIMEDIFF)]

ggplot2::ggplot(data = df.grad, mapping = ggplot2::aes(x = x/60/60, y = y)) +
  ggplot2::geom_raster(mapping = ggplot2::aes(fill = CDF), interpolate = TRUE, alpha = 0.8) +
  ggplot2::scale_fill_gradient2(low = 'white', mid = 'red', high = 'white', midpoint = 0.5) +
  ggplot2::xlab('TIMEDIFF (hour)') + ggplot2::ylab('VALUEDIFF (cmH2O)') +
  ggplot2::ggtitle('Cumulative density gradient of aprior hydrostatic pressure data\nin function of time with 95 % confidence interval.') +
  ggplot2::geom_line(data = df.quant, mapping = ggplot2::aes(y = Q.975), col = 'red') +
  ggplot2::geom_line(data = df.quant, mapping = ggplot2::aes(y = Q.025), col = 'red') +
  ggplot2::scale_x_continuous(breaks = seq(0, 200, by = 24.833/2)) + # lunar day: 24h50
  ggplot2::theme_minimal()
```

### Distribution of errors

Based on the "correct" _a-priori_ timeseries, the densities do not seem to be normal. For example, for $\Delta t = 5 \text{min}$ we have:

```{r hydropressure-5min-hist, echo=FALSE, fig.height=2, fig.width=4}
x <- gwloggeR:::apriori.hydropressure.difference.samples(5*60)
ggplot2::ggplot(data = data.frame('x' = x), 
                mapping = ggplot2::aes_string(x = 'x')) +
  ggplot2::geom_histogram(mapping = ggplot2::aes(y = ..density..), fill = 'black', bins = 100) +
  ggplot2::ylab('Density') +
  ggplot2::theme_light()
```

On the other hand, without tidal effect, the normality seems much more pronounced. For example, $\Delta t = 12 \text{h} 25 \text{min}$:

```{r hydropressure-12h-hist, echo=FALSE, fig.height=2, fig.width=4}
x <- gwloggeR:::apriori.hydropressure.difference.samples(60*60*12 + 60*25)
ggplot2::ggplot(data = data.frame('x' = x), 
                mapping = ggplot2::aes_string(x = 'x')) +
  ggplot2::geom_histogram(mapping = ggplot2::aes(y = ..density..), fill = 'black', bins = 100) +
  ggplot2::ylab('Density') +
  ggplot2::theme_light()
```

This suggest that the model should be adjusted for tidal effects.

## Extensions

The main idea is that $z_t$ is polluted by different kinds of events. We define each of these events separately. Inspiration for these definitions is mainly taken from the works of @Fox1972 and more recent papers by @Chen1993 and @Galeano2013. Furthermore, an R implementation by @tsoutliers was used for the initial evaluation of these ideas.

### Outliers

An additive outlier (AO) at time $t_{AO}$ is defined as an exogenous change of a single observation. 

$$
x_t = z_t + \omega I(t = t_{AO})
$$

The $I()$ is the indicator function. Furthermore, $x_t$ is the observed value, and $z_t$ is the underlying latent random walk as defined previously. $z_t$ is "polluted" in this case with $\omega$ at time $t = t_{AO}$ resulting in observed $x_t$. Rewriting the equation without $z_t$ we get:

$$
x_t = x_{t-1} + \epsilon(\Delta t) + \omega \left[I(t=t_{AO}) - I(t = t_{AO} + 1) \right]
$$

These equations without $z$ we need later for likelihood maximization.

### Levelshifts

A level shift (LS) is an exogenous change that lasts. Its definition is as follows:

$$
x_t = z_t + \frac{\omega I(t = t_{LS})}{1-B}
$$

Note that $B$ is the backshift operator such that $Bx_t = x_{t-1}$ and $BI(t = t_{LS}) = I(t-1 = t_{LS})$. Here the $(1-B)^{-1}$ evaluates to $1 + B + B^2 + B^3 + \dots$. This is nothing more than saying that after $t$ a constant $\omega$ is always added on top of the latent random walk $z_t$. Removing $z_t$ from the equation reveals a more comprehensive form:

$$
x_t = x_{t-1} + \epsilon(\Delta t) + \omega I(t=t_{LS})
$$

### Temporal changes

A temporal change (TC) is an event that decays exponentially with factor $\delta$.

$$
x_t = z_t + \frac{\omega I(t = t_{TC})}{1-\delta B}
$$

We can see this exponential decay more clearly once we rewrite $(1-\delta B)^{-1}$ as $1 + \delta B + \delta^2B^2 + \delta^3B^3 + \dots$. For example, if $\omega$ = 10, this is how it decays in function of time $t$ for $\delta$ = 0.5 and 0.9:

```{r}
10*0.5^(0:5)
10*0.9^(0:5)
```

Note how at each step, respectively 50 % and 90 % of the effect remains from the previous step.

Removing the latent $z_t$ from the equation we end up with:

$$
x_t = x_{t-1} + \epsilon(\Delta t) + \omega \left[ I(t=t_{TC}) - \frac{(1-\delta)I(t = t_{TC} + 1)}{1 - \delta B} \right]
$$

## Model building

Based on AO, LS and TC written in function of $x$ only, one sees that the difference $x_t - x_{t-1} = \epsilon (\Delta t)$ unless AO, LS or TC occurred. Thus our first step in optimization is to take the differences $x_t - x_{t-1}$ and test how likely they are under the distribution of $\epsilon (\Delta t)$. If very unlikely (cf. TODO), then consider the observation a candidate for AO, LS or TC.

These three events can come in all kinds of combinations. It seems difficult, if not impossible, to write a decent if-else structure to catch all the possible configurations. That is why we chose a likelihood based approach.

### Likelihood function

Assuming independence of the error term we write:

$$
L(\mathbf{\theta} ; \mathbf{x}) = p(\mathbf{x} | \mathbf{\theta}) = \prod_{t=2}^{n} p(x_t, x_{t-1} | \mathbf{\theta})
$$

where $\mathbf{x} = (x_1, \dots, x_n)$ and $\mathbf{\theta}$ is the vector of parameters $\omega$'s and $\delta$'s accompanied by their corresponding indexes $t_{AO}$, $t_{LS}$ and $t_{TC}$.

### Stepwise model selection

First problem is that we do not know how many AO, LS and TC events there are. Thus we are naturally led to some form of model selection. Model $M_0$ is a model in which no event occurred (i.e. 0 stands for zero-parameter model). $M_1$ is a model in which an AO or LS event occurred (i.e. a one-parameter model). How do we choose between the two? Well, the one that is more likely wins (i.e. the one for which the likelihood function $L(\mathbf{\theta} ; \mathbf{x})$ results in the highest likelihood.)

In the next step we test whether this $M_1$ model is significantly better than the $M_0$ model. We do this with the likelihood ratio (LR) test. If $M_1$ is significantly better than $M_0$ we continue with $M_1$ and add one extra AO or LS parameter which results in a $M_2$ model. This model also competes with a model in which only one TC event occurred. (This is because a TC event has two parameters $\omega$ and $\delta$, and thus an empty model with a TC event added is automatically a $M_2$ model.) Then the best $M_2$ model is tested for significance against the best $M_1$ model, etc.

Note that $\delta$ varies between $0$ and $1$. If $\delta = 0$ we have an outlier (AO), and if $\delta = 1$ we have a level shift (LS). How do we choose between a TC with $\delta = 0$ and an AO model? Since the TC model uses two parameters to explain the same event as AO with one parameter, an AO model is more parsimonious and thus more likely. Thus, a TC will only be chosen if $\delta$ is significantly different from $0$ and $1$.

In case we have a tie between multiple $M_m$ models with $m$ parameters, then the one for which the sum of the absolute values of parameters is smallest is taken as winner. This is a poor man's simulation of a prior on the parameters: the smaller the parameter, the more likely it is, under equal likelihood.

This model selection procedure is quite involved. Programmaticaly, we define an object `ProgressTable` which keeps track of all the best models $M_0$ up till the last significant one. Besides the forward model selection, we also make use of backward selection (cf. TODO) because adding a parameter sometimes makes an existing parameter insignificant, which opens a new path of possibilities in case the insignificant parameter is removed. The best model $M_m$ is the most likely model with $m$ parameters of all the ones tested, but not necessarily the most likely of all the possible configurations. In practice, although this maxima might be local (i.e. not global), it is taken as a good compromise between computational speed and maximization accuracy.

### Likelihood optimization

Each time we add a parameter, our likelihood function changes and must be optimized for most likely $\mathbf{\theta}$. Using a smoothing kernel for $\epsilon(\Delta t)$ more often than not results in a non-convex likelihood. Assuming $\epsilon(\Delta t)$ to be normal has several advantages: speed, numerical stability (due to $logL(\mathbf{\theta} ; \mathbf{x})$) and "mostly" convexity. Experience shows that mixing AO, LS and TC events sometimes results in non-convex likelihoods.

Further optimization is also made by noting the effect of AO, LS and TC events on $x$. For example, an LS event on $t_{LS}$ has only impact on $x_{t_{LS}}$. An AO event has only impact on $x_{t_{AO}}$ and $x_{t_{AO} + 1}$. And a TC event has only impact on $x_{t_{TC}}$ up until the effect dies out. In other words, we can limit the likelihood optimization to a window of only a few points. (cf. TODO)

### Summary

Algorithm for hydrostatic pressure AO, LS and TC detection can be summarized in pseudo-code as follows:

* Compute $x_t - x_{t-1}$
* Test for outliers based on $\epsilon(\Delta t)$ and note the indexes $t_i$
* For $m$ = 0 .. Inf
    * Using $M_m$ iterate over all possible additions ($t_i$, AO, LS and TC)
        * Compute likelihoods
    * Select $M_{m+1}$ as the model with $m$ parameters and highest likelihood 
* Do while $M_{m+1}$ vs. $M_m$ is significant
* $M_m$ is the best model: extract AO, LS and TC indexes and parameters.

### Examples

The following case shows nicely the "creativity" of the algorithm in selecting the events.

```{r, echo=FALSE}
options(logger.root.data.path = './../../data/raw/')
devtools::load_all('./../../gwloggeR.data', export_all = FALSE, quiet = TRUE)
detach('package:gwloggeR.data')
```

```{r hydropressure-interesting-example, echo=FALSE}
df <- gwloggeR.data::read('pp09-1_120829130801_F5474')$df
data.table::setkey(df, TIMESTAMP_UTC)
invisible(gwloggeR::detect_outliers(x = df$PRESSURE_VALUE, timestamps = df$TIMESTAMP_UTC, 
                                    apriori = gwloggeR::apriori('hydrostatic pressure'), 
                                    plot = TRUE, title = 'pp09-1_120829130801_F5474'))
```

The last three outlier (AO) events are obvious. But the first event is a double event: at the same time we have a level shift (LS) and a temporal change (TC). The level shift causes the drop, and then a temporal change jumps and decays back to the new level. It is the best way the model has found to explain what happened there using the AO, LS and TC events.

```{r hydropressure-interesting-example-2, echo=FALSE}
df <- gwloggeR.data::read('DYLP006X_80773')$df
data.table::setkey(df, TIMESTAMP_UTC)
df <- df[TIMESTAMP_UTC > '2010-10-01' & TIMESTAMP_UTC < '2011-01-01',]
invisible(gwloggeR::detect_outliers(x = df$PRESSURE_VALUE, timestamps = df$TIMESTAMP_UTC, 
                                    apriori = gwloggeR::apriori('hydrostatic pressure'), 
                                    plot = TRUE, title = 'DYLP006X_80773'))
```

There were some major [floods in Flanders in november 2010](https://nl.wikipedia.org/wiki/Overstromingen_in_Belgie_2010). DYLP006X_80773 lies in the 'flooding Dijle', so this is a typical example of a flood event. It is not detected as an event because the differences $x_t - x_{t-1}$ are within the expected $\epsilon (\Delta t)$ range.

```{r hydropressure-interesting-example-3, echo=FALSE}
df <- gwloggeR.data::read('DYLP222A_G4201')$df
data.table::setkey(df, TIMESTAMP_UTC)
df <- df[TIMESTAMP_UTC > '2017-01-01' & TIMESTAMP_UTC < '2017-03-01',]
invisible(gwloggeR::detect_outliers(x = df$PRESSURE_VALUE, timestamps = df$TIMESTAMP_UTC, 
                                    apriori = gwloggeR::apriori('hydrostatic pressure'), 
                                    plot = TRUE, title = 'DYLP222A_G4201'))
```

In this case, a LS event is detected, but not a TC event as one might expect. The reason why is because after the drop, it stays low for several points (LS), and then gradually starts decaying back (TC). Exponential decay is strongest at the start, so TC doesn't fit well for the first couple of points where the decay effect should be the strongest. Eventually, the algorithm chooses a TC or a LS event based on the likelihood, and in this case a LS event has won.

## Future work

* Adjust model for tidal effects.
* Better $\epsilon (\Delta t)$ treshold estimation than current +20 % of min/max value.
* Revise LR test threshold.
* Use _a-priori_ data more as a prior and estimate $\epsilon(\Delta t)$ with data at hand.
* Extract AO, LS and TC if `verbose = TRUE`.
* Revise the 25 observations window.
* Revise parameter significance test.
* Lower threshold based on air pressure for `detect_outliers()`?
* `detect_temporalchanges()` should flag everything until the decay is insignificant.
* Issue #47: if intervals are not divisible by 5min and less than 5min (fixed): small 1min sample is used for $\epsilon(\Delta t)$ estimation.

## References


